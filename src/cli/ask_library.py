#!/usr/bin/env python3
"""
ask_library - Consulta tu biblioteca inteligente con LibrarAI.

Uso:
    python -m src.cli.ask_library "tu pregunta"
    python -m src.cli.ask_library --interactive
    
Ejemplos:
    python -m src.cli.ask_library "¬øQu√© es el algoritmo de Shor?"
    python -m src.cli.ask_library "Compara BB84 con E91" --model claude
    python -m src.cli.ask_library --interactive
"""

import argparse
import logging
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Optional, List

# Configurar logging
logging.basicConfig(
    level=logging.WARNING,
    format='%(levelname)s: %(message)s'
)
logger = logging.getLogger(__name__)


def setup_paths():
    """Configura rutas del proyecto."""
    project_root = Path(__file__).parent.parent.parent
    
    return {
        "indices_dir": project_root / "indices",
        "config_dir": project_root / "config",
        "logs_dir": project_root / "logs",
        "outputs_dir": project_root / "outputs"
    }


def load_config(config_path: Path) -> dict:
    """Carga configuraci√≥n desde YAML."""
    try:
        import yaml
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    except Exception as e:
        logger.warning(f"No se pudo cargar config: {e}")
        return {}


def execute_code_blocks(content: str, outputs_dir: Path):
    """
    Detecta y ejecuta bloques de c√≥digo Python en la respuesta.
    
    Args:
        content: Contenido de la respuesta con posibles bloques ```python
        outputs_dir: Directorio para guardar figuras generadas
    """
    import re
    
    # Buscar bloques de c√≥digo Python
    code_pattern = re.compile(r'```python\n(.*?)```', re.DOTALL)
    code_blocks = code_pattern.findall(content)
    
    if not code_blocks:
        return
    
    print("\n" + "‚ïê" * 60)
    print("üñ•Ô∏è  EJECUCI√ìN DE C√ìDIGO")
    print("‚ïê" * 60)
    
    try:
        from ..execution.sandbox import CodeSandbox
        sandbox = CodeSandbox(
            timeout_seconds=30,
            outputs_dir=outputs_dir
        )
        
        for i, code in enumerate(code_blocks, 1):
            print(f"\n[Bloque {i}]")
            print(f"{'‚îÄ' * 40}")
            
            # Validar antes de ejecutar
            is_valid, error = sandbox.validate_code(code)
            if not is_valid:
                print(f"‚ö†Ô∏è C√≥digo no ejecutado (seguridad): {error}")
                continue
            
            # Preguntar al usuario si quiere ejecutar
            print(f"C√≥digo a ejecutar:")
            print(f"```python\n{code[:500]}{'...' if len(code) > 500 else ''}\n```")
            
            try:
                confirm = input("\n¬øEjecutar este c√≥digo? [s/N]: ").strip().lower()
            except EOFError:
                confirm = 'n'
            
            if confirm not in ['s', 'si', 's√≠', 'y', 'yes']:
                print("‚è≠Ô∏è Omitido")
                continue
            
            print("\n‚è≥ Ejecutando...")
            result = sandbox.execute(code)
            
            if result.success:
                print("‚úÖ Ejecuci√≥n exitosa")
                if result.stdout:
                    print(f"\nüì§ Salida:\n{result.stdout}")
                if result.figures:
                    print(f"\nüìä {len(result.figures)} figura(s) generada(s)")
                    # Guardar figuras
                    for j, fig_b64 in enumerate(result.figures, 1):
                        fig_path = outputs_dir / f"figure_{i}_{j}.png"
                        import base64
                        with open(fig_path, 'wb') as f:
                            f.write(base64.b64decode(fig_b64))
                        print(f"   Guardada: {fig_path}")
            else:
                print(f"‚ùå Error: {result.error_type}")
                if result.stderr:
                    print(f"   {result.stderr[:200]}")
            
            print(f"‚è±Ô∏è Tiempo: {result.execution_time_ms:.0f}ms")
    
    except ImportError as e:
        print(f"‚ö†Ô∏è M√≥dulo de ejecuci√≥n no disponible: {e}")
    except Exception as e:
        print(f"‚ùå Error ejecutando c√≥digo: {e}")
    
    print("\n" + "‚ïê" * 60)


def print_banner():
    """Muestra banner del CLI."""
    banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë        ÔøΩ LibrarAI - Ask Library                           ‚ïë
‚ïë        Tu biblioteca inteligente con IA                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
    print(banner)


def format_response(response, sources, show_sources: bool = True, show_cost: bool = True) -> str:
    """Formatea la respuesta para mostrar."""
    output = []
    
    # Respuesta principal
    output.append("\nüìù Respuesta:")
    output.append("‚îÄ" * 60)
    output.append(response.content)
    output.append("‚îÄ" * 60)
    
    # Mostrar cr√≠tica si existe
    if response.metadata and "critique" in response.metadata:
        critique = response.metadata["critique"]
        output.append("\nüîç EVALUACI√ìN DE CALIDAD:")
        output.append("‚îÄ" * 40)
        
        # Score general
        if "overall_score" in critique:
            score = critique["overall_score"]
            stars = "‚òÖ" * int(score * 5) + "‚òÜ" * (5 - int(score * 5))
            output.append(f"  üìä Score: {score:.2f} {stars}")
        
        # Cobertura de citas
        if "citation_coverage" in critique:
            cov = critique["citation_coverage"]
            output.append(f"  üìñ Citas usadas: {cov*100:.0f}%")
        
        # Fundamentaci√≥n
        if "grounded_score" in critique:
            gs = critique["grounded_score"]
            output.append(f"  üéØ Fundamentaci√≥n: {gs*100:.0f}%")
        
        # Fortalezas
        if critique.get("strengths"):
            output.append("\n  ‚úÖ Fortalezas:")
            for s in critique["strengths"][:3]:
                output.append(f"     ‚Ä¢ {s}")
        
        # Problemas
        if critique.get("issues"):
            output.append("\n  ‚ö†Ô∏è  Problemas detectados:")
            for issue in critique["issues"][:3]:
                if isinstance(issue, dict):
                    severity = issue.get("severity", "?")
                    desc = issue.get("description", "Sin descripci√≥n")
                    icon = "üî¥" if severity == "high" else "üü°" if severity == "medium" else "üü¢"
                    output.append(f"     {icon} {desc}")
                else:
                    output.append(f"     ‚Ä¢ {issue}")
        
        # Sugerencias
        if critique.get("suggestions"):
            output.append("\n  üí° Sugerencias:")
            for sug in critique["suggestions"][:2]:
                output.append(f"     ‚Ä¢ {sug}")
        
        output.append("‚îÄ" * 40)
    
    # Metadatos
    output.append(f"\nüìä Modelo: {response.model}")
    output.append(f"üî¢ Tokens: {response.tokens_input} entrada, {response.tokens_output} salida")
    output.append(f"‚è±Ô∏è  Latencia: {response.latency_ms:.0f}ms")
    
    # Coste estimado
    if show_cost:
        try:
            from ..utils.cost_tracker import get_tracker
            tracker = get_tracker()
            cost = tracker.calculate_cost(
                response.model,
                response.tokens_input,
                response.tokens_output
            )
            output.append(f"üí∞ Coste: ${cost:.4f}")
        except Exception:
            pass
    
    # Fuentes
    if show_sources and sources:
        output.append(f"\nüìö Fuentes ({len(sources)}):")
        for i, source in enumerate(sources[:5], 1):
            title = source.doc_title[:40] + "..." if len(source.doc_title) > 40 else source.doc_title
            output.append(f"   [{i}] {title}")
            output.append(f"       üìç {source.header_path}")
    
    return "\n".join(output)


def save_session(
    query: str,
    response,
    sources,
    output_path: Path
):
    """Guarda la sesi√≥n a un archivo."""
    session = {
        "timestamp": datetime.now().isoformat(),
        "query": query,
        "response": response.to_dict(),
        "sources": [s.to_dict() for s in sources]
    }
    
    # Crear directorio si no existe
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # A√±adir a archivo de sesi√≥n
    sessions_file = output_path / "sessions.jsonl"
    with open(sessions_file, 'a') as f:
        f.write(json.dumps(session, ensure_ascii=False) + "\n")


class RAGPipeline:
    """Pipeline RAG completo."""
    
    def __init__(
        self,
        indices_dir: Path,
        config: dict,
        model: str = "claude-sonnet-4-5-20250929",
        use_router: bool = True,
        use_critic: bool = False,
        use_reranker: bool = False,
        reranker_preset: str = "balanced",
        use_cache: bool = True,
        use_hyde: bool = False,
        hyde_domain: str = "quantum_computing",
        use_semantic_cache: bool = True,
        semantic_cache_threshold: float = 0.92,
        compress_context: bool = False,
        compress_level: str = "medium"
    ):
        self.indices_dir = indices_dir
        self.config = config
        self.model = model
        self.use_router = use_router
        self.use_critic = use_critic
        self.use_reranker = use_reranker
        self.reranker_preset = reranker_preset
        self.use_cache = use_cache
        self.use_hyde = use_hyde
        self.hyde_domain = hyde_domain
        self.use_semantic_cache = use_semantic_cache
        self.semantic_cache_threshold = semantic_cache_threshold
        self.compress_context = compress_context
        self.compress_level = compress_level
        
        # Componentes (lazy init)
        self._retriever = None
        self._synthesizer = None
        self._router = None
        self._critic = None
        self._citation_injector = None
        self._semantic_cache = None
        self._context_compressor = None
    
    def _init_components(self):
        """Inicializa componentes del pipeline."""
        if self._retriever is not None:
            return
        
        from ..retrieval.fusion import UnifiedRetriever
        from ..generation.synthesizer import ResponseSynthesizer
        from ..generation.citation_injector import CitationInjector
        from ..agents.router import QueryRouter
        from ..agents.critic import ResponseCritic
        
        retrieval_config = self.config.get("retrieval", {})
        
        # Determinar si usar reranker: CLI flag tiene prioridad sobre config
        use_reranker = self.use_reranker or retrieval_config.get("reranker", {}).get("enabled", False)
        reranker_preset = self.reranker_preset or retrieval_config.get("reranker", {}).get("preset", "balanced")
        
        # Determinar si usar HyDE: CLI flag tiene prioridad sobre config
        use_hyde = self.use_hyde or retrieval_config.get("hyde", {}).get("enabled", False)
        hyde_domain = self.hyde_domain or retrieval_config.get("hyde", {}).get("domain", "quantum_computing")
        
        self._retriever = UnifiedRetriever(
            indices_dir=self.indices_dir,
            vector_weight=retrieval_config.get("vector_weight", 0.5),
            bm25_weight=retrieval_config.get("bm25_weight", 0.3),
            graph_weight=retrieval_config.get("graph_weight", 0.2),
            use_graph=self.config.get("graph", {}).get("enabled", True),
            use_reranker=use_reranker,
            reranker_preset=reranker_preset,
            use_cache=self.use_cache,
            use_hyde=use_hyde,
            hyde_domain=hyde_domain
        )
        
        gen_config = self.config.get("generation", {})
        
        self._synthesizer = ResponseSynthesizer(
            model=self.model,
            temperature=gen_config.get("temperature", 0.3),
            max_output_tokens=gen_config.get("max_tokens", 2000)
        )
        
        self._citation_injector = CitationInjector()
        
        if self.use_router:
            self._router = QueryRouter(use_llm_router=False)
        
        if self.use_critic:
            self._critic = ResponseCritic(use_llm_critic=False)
        
        # Cach√© sem√°ntico
        if self.use_semantic_cache:
            try:
                from ..retrieval.semantic_cache import (
                    SemanticCache, SemanticCacheConfig
                )
                cache_config = SemanticCacheConfig(
                    similarity_threshold=self.semantic_cache_threshold
                )
                cache_dir = self.indices_dir / "semantic_cache"
                self._semantic_cache = SemanticCache(cache_dir, cache_config)
                logger.debug("Cach√© sem√°ntico inicializado")
            except Exception as e:
                logger.warning(f"No se pudo inicializar cach√© sem√°ntico: {e}")
                self._semantic_cache = None
        
        # Compresor de contexto
        if self.compress_context:
            try:
                from ..generation.context_compressor import (
                    ContextCompressor, CompressionConfig, CompressionLevel
                )
                level_map = {
                    "light": CompressionLevel.LIGHT,
                    "medium": CompressionLevel.MEDIUM,
                    "aggressive": CompressionLevel.AGGRESSIVE
                }
                config = CompressionConfig(
                    level=level_map.get(self.compress_level, CompressionLevel.MEDIUM)
                )
                self._context_compressor = ContextCompressor(config)
                logger.info(f"Compresor de contexto inicializado (nivel: {self.compress_level})")
            except Exception as e:
                logger.warning(f"No se pudo inicializar compresor: {e}")
                self._context_compressor = None
    
    def ask(
        self,
        query: str,
        top_k: int = 10,
        stream: bool = False,
        stream_callback=None,
        sources_only: bool = False,
        abstention_threshold: float = 0.002,
        filters: dict = None
    ):
        """
        Procesa una consulta.
        
        Args:
            query: Pregunta del usuario
            top_k: N√∫mero de fuentes a recuperar
            stream: Si usar streaming
            stream_callback: Callback para streaming
            sources_only: Solo devolver fuentes sin generar respuesta
            abstention_threshold: Umbral m√≠nimo de score RRF para responder (default: 0.002)
                                 Nota: scores RRF t√≠picos est√°n en rango 0.001-0.02
            filters: Filtros de metadatos (ej: {"category": "computacion_cuantica"})
        
        Returns:
            Tuple (response, sources, routing_decision)
        """
        self._init_components()
        
        # 0. Verificar cach√© sem√°ntico
        if self._semantic_cache and not sources_only and not stream:
            try:
                cached, similarity = self._semantic_cache.get(query)
                if cached:
                    logger.info(f"üíæ Cache sem√°ntico HIT (sim={similarity:.3f})")
                    from ..generation.synthesizer import GeneratedResponse
                    
                    # Reconstruir sources desde cach√©
                    from ..retrieval.fusion import RetrievalResult
                    cached_sources = [
                        RetrievalResult(
                            chunk_id=s.get("chunk_id", ""),
                            content=s.get("content", ""),
                            score=s.get("score", 0.0),
                            doc_id=s.get("doc_id", ""),
                            doc_title=s.get("doc_title", ""),
                            header_path=s.get("header_path", ""),
                            retriever_type=s.get("retriever_type", "cached")
                        )
                        for s in cached.sources
                    ]
                    
                    # Reconstruir routing desde cach√©
                    from ..agents.router import RoutingDecision, RetrievalStrategy
                    cached_routing = RoutingDecision(
                        query_type=cached.routing_info.get("query_type", "unknown"),
                        strategy=RetrievalStrategy.HYBRID,
                        reasoning=f"[CACHED] {cached.routing_info.get('reasoning', '')}",
                        vector_weight=cached.routing_info.get("vector_weight", 0.5),
                        bm25_weight=cached.routing_info.get("bm25_weight", 0.3),
                        graph_weight=cached.routing_info.get("graph_weight", 0.2)
                    )
                    
                    return GeneratedResponse(
                        content=cached.response,
                        query=query,
                        query_type=cached.routing_info.get("query_type", "cached"),
                        sources_used=[s.get("chunk_id", "") for s in cached.sources],
                        model=f"{cached.model} (cached)",
                        tokens_input=0,  # No se consumen tokens
                        tokens_output=0,
                        latency_ms=0,
                        metadata={"semantic_cache_hit": True, "similarity": similarity}
                    ), cached_sources, cached_routing
            except Exception as e:
                logger.debug(f"Error consultando cach√© sem√°ntico: {e}")
        
        # 1. Routing (si habilitado)
        routing = None
        if self._router:
            routing = self._router.route(query)
            logger.info(f"Routing: {routing.strategy.value}")
        
        # 2. Retrieval con pesos din√°micos del router
        search_kwargs = {"top_k": top_k}
        if routing:
            search_kwargs["vector_top_k"] = int(top_k * (1 + routing.vector_weight))
            search_kwargs["bm25_top_k"] = int(top_k * (1 + routing.bm25_weight))
            # Pasar pesos din√°micos para la fusi√≥n RRF
            search_kwargs["dynamic_weights"] = {
                "vector": routing.vector_weight,
                "bm25": routing.bm25_weight,
                "graph": routing.graph_weight
            }
        
        if filters:
            search_kwargs["filters"] = filters
        
        sources = self._retriever.search(query, **search_kwargs)
        
        if not sources:
            # Sin fuentes, responder que no hay informaci√≥n
            from ..generation.synthesizer import GeneratedResponse
            return GeneratedResponse(
                content="No encontr√© informaci√≥n relevante en la biblioteca para responder esta consulta.",
                query=query,
                query_type="none",
                sources_used=[],
                model=self.model,
                tokens_input=0,
                tokens_output=0,
                latency_ms=0,
                abstained=True
            ), [], routing
        
        # 2.5 Pol√≠tica de abstenci√≥n: verificar score m√≠nimo (escala RRF)
        # Nota: Scores RRF t√≠picos: 0.001-0.02, un buen match tiene ~0.005+
        max_score = max(s.score for s in sources) if sources else 0
        
        if max_score < abstention_threshold and not sources_only:
            from ..generation.synthesizer import GeneratedResponse
            # Construir mensaje de abstenci√≥n con fuentes cercanas
            closest_sources = sorted(sources, key=lambda x: -x.score)[:3]
            sources_list = "\n".join([
                f"  - {s.doc_title} > {s.header_path} (score: {s.score:.4f})"
                for s in closest_sources
            ])
            
            abstention_msg = (
                f"‚ö†Ô∏è **No encontr√© informaci√≥n suficientemente relevante** para responder con certeza.\n\n"
                f"El score m√°ximo RRF ({max_score:.4f}) est√° por debajo del umbral ({abstention_threshold:.4f}).\n\n"
                f"**Documentos m√°s cercanos (pero no suficientemente relevantes):**\n{sources_list}\n\n"
                f"üí° Sugerencias:\n"
                f"- Reformula la pregunta con t√©rminos m√°s espec√≠ficos\n"
                f"- Verifica que el tema est√© cubierto en tu biblioteca\n"
                f"- Usa `--sources` para explorar qu√© documentos hay disponibles"
            )
            
            return GeneratedResponse(
                content=abstention_msg,
                query=query,
                query_type="abstention",
                sources_used=[],
                model=self.model,
                tokens_input=0,
                tokens_output=0,
                latency_ms=0,
                abstained=True,
                metadata={"max_score": max_score, "threshold": abstention_threshold}
            ), sources, routing
        
        # 2.6 Modo solo fuentes (sin generaci√≥n)
        if sources_only:
            from ..generation.synthesizer import GeneratedResponse
            return GeneratedResponse(
                content="[Modo --sources: solo se muestran fuentes, sin generar respuesta]",
                query=query,
                query_type="sources_only",
                sources_used=[s.chunk_id for s in sources],
                model="none",
                tokens_input=0,
                tokens_output=0,
                latency_ms=0
            ), sources, routing
        
        # 2.7 Compresi√≥n de contexto (si habilitada)
        compressed_sources = sources
        compression_stats = None
        if self._context_compressor and sources:
            try:
                contexts = [s.content for s in sources]
                compressed_contexts, compression_stats = self._context_compressor.compress_contexts(
                    contexts,
                    max_total_tokens=4000  # Presupuesto de tokens para contexto
                )
                
                if compression_stats.get("compression_applied"):
                    # Actualizar sources con contenido comprimido
                    from ..retrieval.fusion import RetrievalResult
                    compressed_sources = [
                        RetrievalResult(
                            chunk_id=s.chunk_id,
                            content=compressed_contexts[i],
                            score=s.score,
                            doc_id=s.doc_id,
                            doc_title=s.doc_title,
                            header_path=s.header_path,
                            retriever_type=s.retriever_type
                        )
                        for i, s in enumerate(sources) if i < len(compressed_contexts)
                    ]
                    logger.info(
                        f"üì¶ Contexto comprimido: {compression_stats['original_tokens']} ‚Üí "
                        f"{compression_stats['compressed_tokens']} tokens "
                        f"({compression_stats['compression_ratio']:.1%})"
                    )
            except Exception as e:
                logger.warning(f"Error comprimiendo contexto: {e}")
                compressed_sources = sources
        
        # 3. Generation (usando sources comprimidas si aplica)
        response = self._synthesizer.generate(
            query=query,
            results=compressed_sources,
            stream=stream,
            stream_callback=stream_callback
        )
        
        # A√±adir stats de compresi√≥n a metadata
        if compression_stats:
            response.metadata["compression"] = compression_stats
        
        # 4. Cr√≠tica (si habilitada)
        if self._critic:
            critique = self._critic.critique(response, sources, query)
            response.metadata["critique"] = critique.to_dict()
        
        # 5. Guardar en cach√© sem√°ntico si est√° habilitado
        if self._semantic_cache and not response.abstained and not stream:
            try:
                # Preparar datos para cach√©
                sources_data = [
                    {
                        "chunk_id": s.chunk_id,
                        "content": s.content,
                        "score": s.score,
                        "doc_id": s.doc_id,
                        "doc_title": s.doc_title,
                        "header_path": s.header_path,
                        "retriever_type": s.retriever_type
                    }
                    for s in sources
                ]
                routing_data = {
                    "query_type": routing.query_type if routing else "unknown",
                    "reasoning": routing.reasoning if routing else "",
                    "vector_weight": routing.vector_weight if routing else 0.5,
                    "bm25_weight": routing.bm25_weight if routing else 0.3,
                    "graph_weight": routing.graph_weight if routing else 0.2
                }
                
                self._semantic_cache.put(
                    query=query,
                    response=response.content,
                    sources=sources_data,
                    routing_info=routing_data,
                    model=response.model,
                    generation_time=response.latency_ms
                )
                logger.info(f"üíæ Respuesta guardada en cach√© sem√°ntico")
            except Exception as e:
                logger.debug(f"Error guardando en cach√© sem√°ntico: {e}")
        
        return response, sources, routing
    
    def ask_deep(
        self,
        query: str,
        top_k: int = 10,
        max_iterations: int = 3,
        min_score_threshold: float = 0.6
    ):
        """
        Modo Deep Research: descompone queries complejas y busca iterativamente.
        
        Args:
            query: Pregunta original
            top_k: N√∫mero de fuentes por sub-query
            max_iterations: M√°ximo de iteraciones de b√∫squeda
            min_score_threshold: Umbral m√≠nimo de score para considerar suficiente
        
        Returns:
            Tuple (response, all_sources, routing_decision)
        """
        self._init_components()
        
        # 1. Analizar complejidad y descomponer si necesario
        routing = self._router.route(query) if self._router else None
        
        # Usar LLM para descomponer query compleja
        sub_queries = self._decompose_query(query, routing)
        
        logger.info(f"Deep Research: {len(sub_queries)} sub-queries")
        print(f"\nüìã Descomponiendo en {len(sub_queries)} sub-preguntas:")
        for i, sq in enumerate(sub_queries, 1):
            print(f"   {i}. {sq}")
        
        # 2. Buscar para cada sub-query
        all_sources = []
        all_source_ids = set()
        
        for iteration in range(max_iterations):
            iteration_sources = []
            
            for sq in sub_queries:
                sources = self._retriever.search(sq, top_k=top_k)
                for s in sources:
                    if s.chunk_id not in all_source_ids:
                        all_source_ids.add(s.chunk_id)
                        iteration_sources.append(s)
            
            all_sources.extend(iteration_sources)
            
            # Evaluar si tenemos suficiente informaci√≥n
            if iteration_sources:
                max_score = max(s.score for s in iteration_sources)
                print(f"\nüîÑ Iteraci√≥n {iteration + 1}: {len(iteration_sources)} nuevas fuentes (max score: {max_score:.2f})")
                
                if max_score >= min_score_threshold and len(all_sources) >= top_k:
                    break
            else:
                break
        
        # 3. Ordenar y limitar fuentes
        all_sources.sort(key=lambda x: -x.score)
        final_sources = all_sources[:top_k * 2]  # M√°s contexto para deep research
        
        if not final_sources:
            from ..generation.synthesizer import GeneratedResponse
            return GeneratedResponse(
                content="No encontr√© informaci√≥n relevante tras b√∫squeda profunda.",
                query=query,
                query_type="deep_research_failed",
                sources_used=[],
                model=self.model,
                tokens_input=0,
                tokens_output=0,
                latency_ms=0,
                abstained=True
            ), [], routing
        
        print(f"\n‚úÖ Total: {len(final_sources)} fuentes relevantes encontradas")
        
        # 4. Generar respuesta con contexto expandido
        response = self._synthesizer.generate(
            query=query,
            results=final_sources,
            stream=False
        )
        
        response.metadata["deep_research"] = {
            "sub_queries": sub_queries,
            "iterations": iteration + 1,
            "total_sources_found": len(all_sources)
        }
        
        # 5. Cr√≠tica
        if self._critic:
            critique = self._critic.critique(response, final_sources, query)
            response.metadata["critique"] = critique.to_dict()
        
        return response, final_sources, routing
    
    def _decompose_query(self, query: str, routing=None) -> List[str]:
        """
        Descompone una query compleja en sub-queries.
        
        Usa heur√≠sticas simples primero, LLM si es necesario.
        """
        sub_queries = [query]  # Siempre incluir original
        
        # Detectar patrones de comparaci√≥n
        comparison_patterns = [
            (r"(?:compara|diferencia|versus|vs\.?)\s+(.+?)\s+(?:y|con|and|with)\s+(.+)", 
             lambda m: [f"¬øQu√© es {m.group(1).strip()}?", f"¬øQu√© es {m.group(2).strip()}?", f"Diferencias entre {m.group(1).strip()} y {m.group(2).strip()}"]),
            (r"(?:relaci√≥n|relationship)\s+(?:entre|between)\s+(.+?)\s+(?:y|and)\s+(.+)",
             lambda m: [f"¬øQu√© es {m.group(1).strip()}?", f"¬øQu√© es {m.group(2).strip()}?", f"¬øC√≥mo se relacionan {m.group(1).strip()} y {m.group(2).strip()}?"]),
        ]
        
        import re
        for pattern, extractor in comparison_patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                sub_queries = extractor(match)
                break
        
        # Detectar m√∫ltiples conceptos (con "y", "adem√°s", etc.)
        if len(sub_queries) == 1:
            multi_patterns = [
                r"(.+?)\s+(?:y adem√°s|y tambi√©n|and also)\s+(.+)",
                r"(.+?)\s+(?:primero|first).+?(?:luego|then)\s+(.+)"
            ]
            for pattern in multi_patterns:
                match = re.search(pattern, query, re.IGNORECASE)
                if match:
                    sub_queries = [match.group(1).strip(), match.group(2).strip(), query]
                    break
        
        # Si el routing indica multi-hop o compleja, intentar descomponer con LLM
        if routing and routing.strategy.value in ["multi_hop", "hybrid_with_graph"]:
            if len(sub_queries) == 1:
                # Intentar descomposici√≥n con LLM ligero
                llm_decomposition = self._decompose_with_llm(query)
                if llm_decomposition:
                    sub_queries = llm_decomposition
        
        return sub_queries
    
    def _decompose_with_llm(self, query: str) -> List[str]:
        """Usa LLM para descomponer query (solo si es realmente necesario)."""
        try:
            import openai
            client = openai.OpenAI()
            
            response = client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[
                    {"role": "system", "content": """Eres un asistente que descompone preguntas complejas en sub-preguntas m√°s simples.
Devuelve SOLO una lista JSON de strings con las sub-preguntas.
Si la pregunta ya es simple, devuelve ["pregunta original"].
M√°ximo 4 sub-preguntas."""},
                    {"role": "user", "content": f"Descomp√≥n esta pregunta: {query}"}
                ],
                temperature=0,
                max_tokens=200
            )
            
            import json
            content = response.choices[0].message.content.strip()
            # Extraer JSON de la respuesta
            if "[" in content:
                json_str = content[content.find("["):content.rfind("]")+1]
                sub_queries = json.loads(json_str)
                if isinstance(sub_queries, list) and all(isinstance(s, str) for s in sub_queries):
                    return sub_queries
        except Exception as e:
            logger.warning(f"Error en descomposici√≥n LLM: {e}")
        
        return None


def interactive_mode(pipeline: RAGPipeline, save_sessions: bool, output_path: Path):
    """Modo interactivo con memoria conversacional."""
    from ..agents.session_manager import get_session_manager
    
    # Inicializar session manager
    session_manager = get_session_manager(output_path / "sessions")
    session_id = session_manager.create_session()
    
    print("\nüéØ Modo interactivo con memoria conversacional")
    print(f"   Sesi√≥n: {session_id[:8]}...")
    print("   Comandos especiales:")
    print("   - /sources    Ver fuentes de la √∫ltima respuesta")
    print("   - /export     Exportar √∫ltima respuesta a Markdown")
    print("   - /history    Ver historial de conversaci√≥n")
    print("   - /clear      Limpiar pantalla")
    print("   - /new        Nueva sesi√≥n (borrar memoria)")
    print("")
    print("   üí° Soporta preguntas de seguimiento:")
    print('      "M√°s detalles", "Expande el punto 2", "¬øY si...?"')
    print("")
    
    last_response = None
    last_sources = None
    
    while True:
        try:
            query = input("\n‚ùì Tu pregunta: ").strip()
            
            if not query:
                continue
            
            if query.lower() in ['salir', 'exit', 'quit', 'q']:
                print("\nüëã ¬°Hasta pronto!")
                break
            
            # Comandos especiales
            if query.startswith('/'):
                cmd = query[1:].lower()
                
                if cmd == 'sources' and last_sources:
                    print("\nüìö Fuentes de la √∫ltima respuesta:")
                    for i, s in enumerate(last_sources, 1):
                        print(f"\n[{i}] {s.doc_title}")
                        print(f"    {s.header_path}")
                        print(f"    Score: {s.score:.3f}")
                        print(f"    Preview: {s.content[:200]}...")
                    continue
                
                elif cmd == 'export' and last_response:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    export_path = output_path / f"response_{timestamp}.md"
                    export_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    with open(export_path, 'w') as f:
                        f.write(f"# Consulta\n\n{last_response.query}\n\n")
                        f.write(f"# Respuesta\n\n{last_response.content}\n\n")
                        f.write(f"# Fuentes\n\n")
                        for i, s in enumerate(last_sources, 1):
                            f.write(f"- [{i}] {s.doc_title} - {s.header_path}\n")
                    
                    print(f"‚úÖ Exportado a {export_path}")
                    continue
                
                elif cmd == 'history':
                    context = session_manager.get_session(session_id)
                    if context and context.messages:
                        print("\nüìú Historial de conversaci√≥n:")
                        print("‚îÄ" * 50)
                        for i, msg in enumerate(context.messages, 1):
                            role = "üë§ Usuario" if msg.role == "user" else "ü§ñ Asistente"
                            content = msg.content[:200] + "..." if len(msg.content) > 200 else msg.content
                            print(f"\n[{i}] {role}:")
                            print(f"    {content}")
                        print("\n" + "‚îÄ" * 50)
                    else:
                        print("üì≠ Sin historial a√∫n")
                    continue
                
                elif cmd == 'new':
                    session_id = session_manager.create_session()
                    print(f"üÜï Nueva sesi√≥n: {session_id[:8]}...")
                    last_response = None
                    last_sources = None
                    continue
                
                elif cmd == 'clear':
                    import os
                    os.system('clear' if os.name == 'posix' else 'cls')
                    print_banner()
                    continue
                
                else:
                    print("‚ùå Comando no reconocido")
                    continue
            
            # Detectar si es pregunta de seguimiento
            is_followup, followup_type = session_manager.is_followup_query(query, session_id)
            
            if is_followup:
                expanded_query = session_manager.expand_query_with_context(
                    query, session_id, followup_type
                )
                if expanded_query != query:
                    print(f"üìù Interpretado como: {expanded_query}")
                query_to_search = expanded_query
            else:
                query_to_search = query
            
            # A√±adir mensaje del usuario a la sesi√≥n
            session_manager.add_message(session_id, "user", query)
            
            # Procesar consulta
            print("\nüîç Buscando en la biblioteca...")
            
            response, sources, routing = pipeline.ask(query_to_search)
            
            print(format_response(response, sources))
            
            # A√±adir respuesta a la sesi√≥n
            source_ids = [s.chunk_id for s in sources]
            session_manager.add_message(
                session_id, 
                "assistant", 
                response.content,
                sources=source_ids,
                metadata={"model": response.model}
            )
            
            last_response = response
            last_sources = sources
            
            # Guardar sesi√≥n (ya se guarda autom√°ticamente)
            if save_sessions:
                save_session(query, response, sources, output_path)
            
        except KeyboardInterrupt:
            print("\n\nüëã ¬°Hasta pronto!")
            break
        except Exception as e:
            logger.error(f"Error: {e}")
            print(f"\n‚ùå Error procesando consulta: {e}")


def main():
    """Punto de entrada principal."""
    parser = argparse.ArgumentParser(
        description="Consulta tu biblioteca inteligente usando LibrarAI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos:
  %(prog)s "¬øQu√© es el algoritmo de Shor?"
  %(prog)s "Compara BB84 con E91" --model gpt-4.1 --deep
  %(prog)s "Entrelazamiento cu√°ntico" --sources
  %(prog)s "Calcula la entrop√≠a de von Neumann" --exec
  %(prog)s --interactive
  %(prog)s "Explica el entrelazamiento" --critic --save
"""
    )
    
    parser.add_argument(
        'query',
        nargs='?',
        help='Pregunta a realizar'
    )
    
    parser.add_argument(
        '--interactive', '-i',
        action='store_true',
        help='Modo interactivo'
    )
    
    parser.add_argument(
        '--model', '-m',
        choices=['claude', 'gpt-4.1', 'gpt-4.1-mini', 'local'],
        default='claude',
        help='Modelo a usar (default: claude = Claude Sonnet 4.5)'
    )
    
    parser.add_argument(
        '--top-k', '-k',
        type=int,
        default=10,
        help='N√∫mero de documentos a recuperar (default: 10)'
    )
    
    parser.add_argument(
        '--no-sources',
        action='store_true',
        help='No mostrar fuentes'
    )
    
    parser.add_argument(
        '--sources',
        action='store_true',
        help='Solo mostrar fuentes relevantes sin generar respuesta (ahorra costes)'
    )
    
    parser.add_argument(
        '--deep',
        action='store_true',
        help='Modo Deep Research: descompone queries complejas y busca iterativamente'
    )
    
    parser.add_argument(
        '--stream',
        action='store_true',
        help='Streaming de respuesta'
    )
    
    parser.add_argument(
        '--save', '-s',
        action='store_true',
        help='Guardar sesi√≥n'
    )
    
    parser.add_argument(
        '--json',
        action='store_true',
        help='Salida en formato JSON'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Modo verboso'
    )
    
    parser.add_argument(
        '--no-router',
        action='store_true',
        help='Desactivar router inteligente'
    )
    
    parser.add_argument(
        '--critic',
        action='store_true',
        help='Activar cr√≠tico de respuestas con validaci√≥n de citas'
    )
    
    parser.add_argument(
        '--exec',
        action='store_true',
        help='Permitir ejecuci√≥n de c√≥digo (c√°lculos, gr√°ficas, simulaciones)'
    )
    
    parser.add_argument(
        '--rerank',
        action='store_true',
        help='Aplicar re-ranking con cross-encoder para mejorar precisi√≥n (+15-25%%)'
    )
    
    parser.add_argument(
        '--rerank-preset',
        choices=['fast', 'balanced', 'quality', 'max_quality'],
        default='balanced',
        help='Preset del reranker: fast (r√°pido), balanced (default), quality, max_quality'
    )
    
    parser.add_argument(
        '--no-cache',
        action='store_true',
        help='Deshabilitar cache de embeddings'
    )
    
    parser.add_argument(
        '--filter', '-f',
        type=str,
        action='append',
        dest='filters',
        metavar='KEY:VALUE',
        help='Filtrar por metadatos (ej: --filter category:computacion_cuantica). Puede usarse m√∫ltiples veces'
    )
    
    parser.add_argument(
        '--list-categories',
        action='store_true',
        help='Listar categor√≠as disponibles y salir'
    )
    
    parser.add_argument(
        '--hyde',
        action='store_true',
        help='Activar HyDE (Hypothetical Document Embeddings) para mejorar recall (+10-20%% en queries abstractas)'
    )
    
    parser.add_argument(
        '--hyde-domain',
        choices=['quantum_computing', 'quantum_information', 'quantum_cryptography', 'general_physics', 'mathematics'],
        default='quantum_computing',
        help='Dominio para generaci√≥n HyDE (default: quantum_computing)'
    )
    
    parser.add_argument(
        '--compress',
        action='store_true',
        help='Comprimir contexto para incluir m√°s chunks en el presupuesto de tokens (reduce 30-50%%)'
    )
    
    parser.add_argument(
        '--compress-level',
        choices=['light', 'medium', 'aggressive'],
        default='medium',
        help='Nivel de compresi√≥n de contexto: light (~20%%), medium (~40%%), aggressive (~60%%)'
    )
    
    parser.add_argument(
        '--cache-stats',
        action='store_true',
        help='Mostrar estad√≠sticas del cache de embeddings'
    )
    
    parser.add_argument(
        '--semantic-cache',
        action='store_true',
        default=True,
        help='Activar cach√© sem√°ntico (default: activado). Reutiliza respuestas para queries similares'
    )
    
    parser.add_argument(
        '--no-semantic-cache',
        action='store_true',
        help='Desactivar cach√© sem√°ntico (fuerza generaci√≥n nueva)'
    )
    
    parser.add_argument(
        '--cache-threshold',
        type=float,
        default=0.92,
        help='Umbral de similitud para cache sem√°ntico (default: 0.92). Valores m√°s altos = m√°s estricto'
    )
    
    parser.add_argument(
        '--semantic-cache-stats',
        action='store_true',
        help='Mostrar estad√≠sticas del cach√© sem√°ntico y salir'
    )
    
    parser.add_argument(
        '--clear-semantic-cache',
        action='store_true',
        help='Limpiar el cach√© sem√°ntico y salir'
    )
    
    parser.add_argument(
        '--costs', '-c',
        action='store_true',
        help='Mostrar resumen de costes acumulados de consultas'
    )
    
    args = parser.parse_args()
    
    # Configurar logging
    if args.verbose:
        logging.getLogger().setLevel(logging.INFO)
    
    # Si solo pide costes, mostrar y salir
    if args.costs and not args.query and not args.interactive:
        try:
            from ..utils.cost_tracker import get_tracker
            tracker = get_tracker()
            tracker.print_summary()  # Sin filtro, muestra todo
        except Exception as e:
            print(f"‚ùå Error mostrando costes: {e}")
        sys.exit(0)
    
    # Si solo pide estad√≠sticas de cache, mostrar y salir
    if args.cache_stats and not args.query and not args.interactive:
        try:
            paths = setup_paths()
            from ..retrieval.cache import EmbeddingCache
            cache = EmbeddingCache(indices_dir=paths["indices_dir"])
            stats = cache.get_stats()
            print("\nüìä Estad√≠sticas del Cache de Embeddings")
            print("=" * 45)
            for key, value in stats.items():
                print(f"  {key}: {value}")
            cache.close()
        except Exception as e:
            print(f"‚ùå Error mostrando estad√≠sticas de cache: {e}")
        sys.exit(0)
    
    # Si pide estad√≠sticas del cach√© sem√°ntico
    if args.semantic_cache_stats and not args.query and not args.interactive:
        try:
            paths = setup_paths()
            from ..retrieval.semantic_cache import SemanticCache
            cache_dir = paths["indices_dir"] / "semantic_cache"
            if cache_dir.exists():
                cache = SemanticCache(cache_dir)
                stats = cache.get_stats()
                print("\nüíæ Estad√≠sticas del Cach√© Sem√°ntico")
                print("=" * 45)
                for key, value in stats.items():
                    if key == "hit_rate" and value is not None:
                        print(f"  {key}: {value:.1%}")
                    else:
                        print(f"  {key}: {value}")
                cache.close()
            else:
                print("\nüíæ Cach√© sem√°ntico no inicializado a√∫n")
                print("   Se crear√° con la primera consulta")
        except Exception as e:
            print(f"‚ùå Error mostrando estad√≠sticas: {e}")
        sys.exit(0)
    
    # Si pide limpiar el cach√© sem√°ntico
    if args.clear_semantic_cache and not args.query and not args.interactive:
        try:
            paths = setup_paths()
            from ..retrieval.semantic_cache import SemanticCache
            cache_dir = paths["indices_dir"] / "semantic_cache"
            if cache_dir.exists():
                cache = SemanticCache(cache_dir)
                cleared = cache.clear()
                print(f"\nüóëÔ∏è Cach√© sem√°ntico limpiado: {cleared} entradas eliminadas")
                cache.close()
            else:
                print("\nüíæ Cach√© sem√°ntico no existe")
        except Exception as e:
            print(f"‚ùå Error limpiando cach√©: {e}")
        sys.exit(0)
    
    # Si pide listar categor√≠as, mostrar y salir
    if args.list_categories:
        try:
            paths = setup_paths()
            from qdrant_client import QdrantClient
            client = QdrantClient(path=str(paths["indices_dir"] / "qdrant"))
            
            # Obtener categor√≠as √∫nicas haciendo scroll
            categories = set()
            offset = None
            while True:
                results, offset = client.scroll(
                    collection_name="quantum_library",
                    limit=1000,
                    offset=offset,
                    with_payload=["category"]
                )
                for point in results:
                    if point.payload and "category" in point.payload:
                        categories.add(point.payload["category"])
                if offset is None:
                    break
            
            print("\nüìÅ Categor√≠as Disponibles")
            print("=" * 35)
            for cat in sorted(categories):
                print(f"  ‚Ä¢ {cat}")
            print(f"\nTotal: {len(categories)} categor√≠as")
            print("\nUso: --filter category:CATEGORIA")
        except Exception as e:
            print(f"‚ùå Error listando categor√≠as: {e}")
        sys.exit(0)
    
    # Verificar que hay query o modo interactivo
    if not args.query and not args.interactive:
        parser.print_help()
        sys.exit(1)
    
    # Cargar variables de entorno
    try:
        from dotenv import load_dotenv
        env_path = Path(__file__).parent.parent.parent / '.env'
        if env_path.exists():
            load_dotenv(env_path)
    except ImportError:
        pass
    
    # Configurar rutas
    paths = setup_paths()
    
    # Verificar que existen los √≠ndices
    if not paths["indices_dir"].exists():
        print("‚ùå No se encontraron √≠ndices. Ejecuta primero:")
        print("   python -m src.cli.ingest_library")
        sys.exit(1)
    
    # Cargar configuraci√≥n
    config = load_config(paths["config_dir"] / "settings.yaml")
    
    # Mapear modelo
    model_map = {
        "claude": "claude-sonnet-4-5-20250929",  # Claude Sonnet 4.5
        "gpt-4.1": "gpt-4.1",
        "gpt-4.1-mini": "gpt-4.1-mini",
        "local": "llama3.2"  # Ollama
    }
    model = model_map.get(args.model, args.model)
    
    # Determinar si usar cach√© sem√°ntico
    use_semantic_cache = not args.no_semantic_cache
    
    # Determinar si usar compresi√≥n
    compress_context = args.compress
    compress_level = args.compress_level
    
    # Crear pipeline
    try:
        pipeline = RAGPipeline(
            indices_dir=paths["indices_dir"],
            config=config,
            model=model,
            use_router=not args.no_router,
            use_critic=args.critic,
            use_reranker=args.rerank,
            reranker_preset=args.rerank_preset,
            use_cache=not args.no_cache,
            use_hyde=args.hyde,
            hyde_domain=args.hyde_domain,
            use_semantic_cache=use_semantic_cache,
            semantic_cache_threshold=args.cache_threshold,
            compress_context=compress_context,
            compress_level=compress_level
        )
    except Exception as e:
        logger.error(f"Error inicializando pipeline: {e}")
        sys.exit(1)
    
    # Modo interactivo
    if args.interactive:
        print_banner()
        interactive_mode(
            pipeline, 
            args.save, 
            paths["outputs_dir"]
        )
        return
    
    # Modo single query
    if not args.json:
        print_banner()
        print(f"‚ùì Pregunta: {args.query}")
        if args.sources:
            print("\nüîç Buscando fuentes (sin generar respuesta)...")
        elif args.deep:
            print("\nüî¨ Modo Deep Research: analizando consulta...")
        else:
            print("\nüîç Buscando en la biblioteca...")
    
    try:
        # Callback para streaming
        stream_callback = None
        if args.stream and not args.json and not args.sources:
            def stream_callback(text):
                print(text, end='', flush=True)
            print("\nüìù Respuesta:")
            print("‚îÄ" * 60)
        
        # Parsear filtros de args
        filters = None
        if args.filters:
            filters = {}
            for f in args.filters:
                if ':' in f:
                    key, value = f.split(':', 1)
                    filters[key.strip()] = value.strip()
                else:
                    logger.warning(f"Filtro ignorado (formato inv√°lido): {f}")
        
        # Llamar al pipeline con modo deep research si aplica
        if args.deep:
            response, sources, routing = pipeline.ask_deep(
                args.query,
                top_k=args.top_k,
                max_iterations=3
            )
        else:
            response, sources, routing = pipeline.ask(
                args.query,
                top_k=args.top_k,
                stream=args.stream,
                stream_callback=stream_callback,
                sources_only=args.sources,
                filters=filters
            )
        
        if args.json:
            output = {
                "query": args.query,
                "response": response.to_dict(),
                "sources": [s.to_dict() for s in sources],
                "routing": routing.to_dict() if routing else None
            }
            print(json.dumps(output, ensure_ascii=False, indent=2))
        elif args.sources:
            # Modo solo fuentes: mostrar fuentes detalladas
            print("\n" + "‚ïê" * 60)
            print("üìö FUENTES RELEVANTES")
            print("‚ïê" * 60)
            for i, s in enumerate(sources, 1):
                print(f"\n[{i}] {s.doc_title}")
                print(f"    üìç {s.header_path}")
                print(f"    üìä Score: {s.score:.3f} | Nivel: {s.level}")
                print(f"    üìù Preview:")
                preview = s.content[:300].replace('\n', ' ')
                print(f"       {preview}...")
            print("\n" + "‚ïê" * 60)
            print(f"üìà Total: {len(sources)} fuentes | Max score: {max(s.score for s in sources):.3f}")
            print("üí° Usa sin --sources para generar una respuesta completa")
        else:
            if not args.stream:
                print(format_response(response, sources, not args.no_sources))
            else:
                print("\n" + "‚îÄ" * 60)
                print(f"\nüìä Modelo: {response.model}")
                print(f"‚è±Ô∏è  Latencia: {response.latency_ms:.0f}ms")
        
        # Ejecutar c√≥digo si se solicita y la respuesta contiene bloques de c√≥digo
        if args.exec and not args.sources:
            execute_code_blocks(response.content, paths["outputs_dir"])
        
        # Guardar sesi√≥n
        if args.save:
            save_session(args.query, response, sources, paths["outputs_dir"])
            if not args.json:
                print(f"\nüíæ Sesi√≥n guardada en {paths['outputs_dir']}")
        
        # Mostrar costes si se solicita
        if args.costs and not args.json:
            try:
                from ..utils.cost_tracker import get_tracker
                tracker = get_tracker()
                print("\n" + "‚îÄ" * 60)
                tracker.print_summary(usage_type="QUERY")
            except Exception:
                pass
        
    except Exception as e:
        if args.json:
            print(json.dumps({"error": str(e)}))
        else:
            logger.error(f"Error: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
