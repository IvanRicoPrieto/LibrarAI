# ï¿½ LibrarAI - Tu Biblioteca Inteligente

Sistema RAG (Retrieval-Augmented Generation) para consultar tu biblioteca de FÃ­sica, MatemÃ¡ticas y cualquier otra Ã¡rea del conocimiento.

## ğŸ“‹ CaracterÃ­sticas

- **ğŸ” BÃºsqueda hÃ­brida**: Vector (semÃ¡ntica) + BM25 (lÃ©xica) + Grafo (relaciones)
- **ğŸ¯ Re-ranking**: Cross-Encoder opcional que mejora precisiÃ³n +15-25%
- **ğŸ“ EvaluaciÃ³n RAGAS**: Pipeline de evaluaciÃ³n con mÃ©tricas de calidad RAG
- **ğŸ’¾ Cache de Embeddings**: Reduce costes 70-90% y elimina latencia en queries repetidas
- **ğŸ’° CachÃ© SemÃ¡ntico**: Reutiliza respuestas para queries similares (100% ahorro por hit)
- **ğŸ“¦ CompresiÃ³n de Contexto**: Reduce tokens 30-60%, permite mÃ¡s contexto por consulta
- **ğŸ§  Chunking SemÃ¡ntico**: Detecta lÃ­mites naturales (definiciones, teoremas, demostraciones)
- **ğŸ“š Chunking jerÃ¡rquico**: 3 niveles (Macro/Meso/Micro) con auto-merge inteligente
- **ğŸ“ Citas precisas**: Referencias `[n]` a fuentes especÃ­ficas con ubicaciÃ³n
- **ğŸ¤– Multi-LLM**: Claude Sonnet 4.5, GPT-4.1, modelos locales (Ollama)
- **âš¡ IndexaciÃ³n incremental**: Solo procesa documentos nuevos/modificados
- **ğŸ•¸ï¸ Grafo de conocimiento**: ExtracciÃ³n automÃ¡tica de entidades y relaciones
- **ğŸ”¬ Deep Research**: DescomposiciÃ³n de queries complejas con bÃºsqueda iterativa
- **âœ… ValidaciÃ³n de citas**: Critic que verifica que las citas tienen soporte real
- **ğŸ–¥ï¸ Code Sandbox**: EjecuciÃ³n segura de cÃ³digo Python para cÃ¡lculos y grÃ¡ficas

## ğŸš€ InstalaciÃ³n

### 1. Clonar/Copiar el proyecto

```bash
cd "/home/ivan/ComputaciÃ³n CuÃ¡ntica/LibrarAI"
```

### 2. Crear entorno virtual

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# o: .venv\Scripts\activate  # Windows
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar API keys

```bash
cp .env.example .env
# Editar .env con tus API keys
```

Contenido de `.env`:

```
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
```

### 5. Indexar la biblioteca

```bash
python -m src.cli.ingest_library
```

## ğŸ“– Uso

### Consulta simple

```bash
python -m src.cli.ask_library "Â¿QuÃ© es el algoritmo de Shor?"
```

### Modo interactivo

```bash
python -m src.cli.ask_library --interactive
```

### Opciones avanzadas

```bash
# Usar GPT-4.1 en lugar de Claude
python -m src.cli.ask_library "Explica BB84" --model gpt-4.1

# Solo ver fuentes (sin generar respuesta, ahorra costes)
python -m src.cli.ask_library "BB84" --sources

# Deep Research para queries complejas
python -m src.cli.ask_library "Compara BB84 con E91" --deep

# Con validaciÃ³n de citas (Critic)
python -m src.cli.ask_library "Â¿QuÃ© es un qubit?" --critic

# Re-ranking con Cross-Encoder (+15-25% precisiÃ³n)
python -m src.cli.ask_library "Teorema de No-ClonaciÃ³n" --rerank

# Re-ranking con preset de mÃ¡xima calidad
python -m src.cli.ask_library "EcuaciÃ³n de SchrÃ¶dinger" --rerank --rerank-preset quality

# HyDE para mejorar recall (+10-20% en queries abstractas)
python -m src.cli.ask_library "Â¿CÃ³mo funciona la teleportaciÃ³n cuÃ¡ntica?" --hyde

# HyDE con dominio especÃ­fico
python -m src.cli.ask_library "Protocolos de distribuciÃ³n de claves" --hyde --hyde-domain quantum_cryptography

# Combinar HyDE + Re-ranking (mÃ¡xima calidad)
python -m src.cli.ask_library "Deriva la ecuaciÃ³n de SchrÃ¶dinger" --hyde --rerank

# CompresiÃ³n de contexto (permite mÃ¡s chunks en el presupuesto de tokens)
python -m src.cli.ask_library "Compara todos los protocolos QKD" --compress --top-k 20

# CompresiÃ³n agresiva (reduce 60% de tokens)
python -m src.cli.ask_library "Resumen completo del algoritmo de Shor" --compress --compress-level aggressive

# Ejecutar cÃ³digo de la respuesta
python -m src.cli.ask_library "Calcula entropÃ­a de von Neumann" --exec

# MÃ¡s contexto (15 chunks)
python -m src.cli.ask_library "Compara protocolos QKD" --top-k 15

# Streaming de respuesta
python -m src.cli.ask_library "Â¿QuÃ© es el entrelazamiento?" --stream

# Guardar sesiÃ³n
python -m src.cli.ask_library "Deriva la ecuaciÃ³n de SchrÃ¶dinger" --save

# Salida JSON
python -m src.cli.ask_library "Â¿QuÃ© es un qubit?" --json
```

### EvaluaciÃ³n de Calidad (RAGAS)

```bash
# Evaluar una query individual
python -m src.cli.evaluate --query "Â¿QuÃ© es el entrelazamiento cuÃ¡ntico?"

# Ejecutar benchmark completo
python -m src.cli.evaluate --suite default

# Comparar con baseline anterior
python -m src.cli.evaluate --suite default --baseline benchmark_results/baseline.json

# Benchmark sin reranking (para comparaciÃ³n A/B)
python -m src.cli.evaluate --suite default --no-rerank
```

### Cache de Embeddings

```bash
# Ver estadÃ­sticas del cache
python -m src.cli.ask_library --cache-stats

# Deshabilitar cache (Ãºtil para debugging)
python -m src.cli.ask_library "Pregunta" --no-cache
```

### Filtrado por CategorÃ­a

```bash
# Listar categorÃ­as disponibles
python -m src.cli.ask_library --list-categories

# Filtrar por categorÃ­a
python -m src.cli.ask_library "Â¿QuÃ© es un qubit?" --filter category:computacion_cuantica

# MÃºltiples filtros
python -m src.cli.ask_library "BB84" --filter category:comunicacion_cuantica --filter doc_title:Nielsen
```

### HyDE (Query Expansion)

HyDE (Hypothetical Document Embeddings) mejora el recall generando documentos hipotÃ©ticos que responderÃ­an la pregunta, y luego buscando documentos similares. Especialmente Ãºtil para:

- Queries abstractas o conceptuales
- Preguntas que no contienen tÃ©rminos tÃ©cnicos exactos
- BÃºsquedas exploratorias

```bash
# Activar HyDE
python -m src.cli.ask_library "Â¿CÃ³mo se mantiene la coherencia cuÃ¡ntica?" --hyde

# HyDE con dominio especÃ­fico
python -m src.cli.ask_library "Seguridad en QKD" --hyde --hyde-domain quantum_cryptography

# Dominios disponibles: quantum_computing, quantum_information, quantum_cryptography, general_physics, mathematics
```

### Qdrant en Docker (Recomendado para >20K chunks)

```bash
# Iniciar Qdrant en Docker
docker compose up -d

# Configurar URL en .env
echo "QDRANT_URL=http://localhost:6333" >> .env

# Re-indexar la biblioteca (migrarÃ¡ a Docker)
python -m src.cli.ingest_library --force

# Acceder al dashboard
open http://localhost:6333/dashboard
```

### Memoria Conversacional (Follow-up Questions)

El modo interactivo soporta preguntas de seguimiento que mantienen contexto de la conversaciÃ³n anterior:

```bash
# Iniciar modo interactivo con memoria
python -m src.cli.ask_library --interactive

# Ejemplo de conversaciÃ³n:
# â“ Tu pregunta: Â¿QuÃ© es el algoritmo de Shor?
# [respuesta sobre Shor]
# â“ Tu pregunta: Â¿Y quÃ© complejidad tiene?          # Sabe que hablas de Shor
# â“ Tu pregunta: Expande el punto 2                  # AmplÃ­a punto especÃ­fico
# â“ Tu pregunta: Dame un ejemplo mÃ¡s detallado      # MÃ¡s ejemplos del tema
```

**Comandos especiales del modo interactivo:**

| Comando    | DescripciÃ³n                          |
| ---------- | ------------------------------------ |
| `/sources` | Ver fuentes de la Ãºltima respuesta   |
| `/export`  | Exportar Ãºltima respuesta a Markdown |
| `/history` | Ver historial de conversaciÃ³n        |
| `/new`     | Nueva sesiÃ³n (borrar memoria)        |
| `/clear`   | Limpiar pantalla                     |
| `salir`    | Terminar sesiÃ³n                      |

**Tipos de preguntas de seguimiento soportadas:**

- **ExpansiÃ³n**: "MÃ¡s detalles", "Expande el punto 3", "Profundiza en esto"
- **ClarificaciÃ³n**: "Â¿QuÃ© significa X?", "Â¿Puedes aclarar eso?"
- **ComparaciÃ³n**: "Â¿En quÃ© se diferencia de Y?", "Compara con Z"
- **Ejemplo**: "Dame un ejemplo", "Â¿Puedes ilustrar esto?"
- **ContinuaciÃ³n**: "Â¿Y despuÃ©s?", "Â¿QuÃ© mÃ¡s?", "ContinÃºa"
- **Referencia**: "Â¿Y si cambio X?", "Â¿QuÃ© pasa con Y?"

### CachÃ© SemÃ¡ntico

El cachÃ© semÃ¡ntico detecta queries semÃ¡nticamente similares y reutiliza respuestas previas, reduciendo costes de LLM dramÃ¡ticamente:

```bash
# Primera consulta (genera respuesta con LLM)
python -m src.cli.ask_library "Â¿QuÃ© es el entrelazamiento cuÃ¡ntico?"

# Segunda consulta similar (usa cachÃ©, 0 tokens)
python -m src.cli.ask_library "ExplÃ­came el entrelazamiento"

# Ver estadÃ­sticas del cachÃ©
python -m src.cli.ask_library --semantic-cache-stats

# Desactivar cachÃ© (forzar regeneraciÃ³n)
python -m src.cli.ask_library "Â¿QuÃ© es el entrelazamiento?" --no-semantic-cache

# Ajustar umbral de similitud (mÃ¡s estricto)
python -m src.cli.ask_library "Pregunta" --cache-threshold 0.95

# Limpiar cachÃ©
python -m src.cli.ask_library --clear-semantic-cache
```

**CaracterÃ­sticas:**

- Usa embeddings OpenAI (text-embedding-3-small) para comparaciÃ³n semÃ¡ntica
- Umbral configurable (default: 0.92 = 92% similitud)
- TTL de 7 dÃ­as por defecto
- Almacena respuesta + fuentes + routing para reproducibilidad perfecta
- Cache hit = 0 tokens consumidos (100% ahorro en esa query)

### CompresiÃ³n de Contexto

Comprime el contexto para incluir mÃ¡s informaciÃ³n en el presupuesto de tokens del LLM:

```bash
# CompresiÃ³n media (default: ~40% reducciÃ³n)
python -m src.cli.ask_library "Resumen de todos los protocolos QKD" --compress --top-k 20

# CompresiÃ³n ligera (~20% reducciÃ³n, preserva mÃ¡s detalle)
python -m src.cli.ask_library "Explica BB84" --compress --compress-level light

# CompresiÃ³n agresiva (~60% reducciÃ³n, para sÃ­ntesis amplias)
python -m src.cli.ask_library "Estado del arte en computaciÃ³n cuÃ¡ntica" --compress --compress-level aggressive
```

**Niveles de compresiÃ³n:**

| Nivel      | ReducciÃ³n | Caso de uso                        |
| ---------- | --------- | ---------------------------------- |
| light      | ~20%      | Limpieza bÃ¡sica, preserva detalles |
| medium     | ~40%      | Balance entre cobertura y detalle  |
| aggressive | ~60%      | SÃ­ntesis amplia, muchas fuentes    |

**Elementos preservados:**

- FÃ³rmulas LaTeX (`$...$`, `$$...$$`)
- Bloques de cÃ³digo
- Marcadores de cita `[n]`
- Palabras clave tÃ©cnicas (qubit, entanglement, etc.)

## ğŸ—ï¸ Arquitectura

```
LibrarAI/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.yaml      # ConfiguraciÃ³n principal
â”‚   â””â”€â”€ ontology.yaml      # OntologÃ­a del dominio (18 tipos, 19 relaciones)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ books/             # Libros organizados por temÃ¡tica
â”‚   â”‚   â”œâ”€â”€ computacion_cuantica/
â”‚   â”‚   â”œâ”€â”€ computacion_neuromorfica/
â”‚   â”‚   â”œâ”€â”€ comunicacion_cuantica/
â”‚   â”‚   â”œâ”€â”€ espacios_de_hilbert/
â”‚   â”‚   â”œâ”€â”€ estructuras_algebraicas/
â”‚   â”‚   â”œâ”€â”€ geometrias_lineales/
â”‚   â”‚   â”œâ”€â”€ informacion_cuantica/
â”‚   â”‚   â”œâ”€â”€ mecanica_cuantica/
â”‚   â”‚   â”œâ”€â”€ teoria_informacion/
â”‚   â”‚   â””â”€â”€ topologia/
â”‚   â””â”€â”€ papers/            # Papers organizados por temÃ¡tica
â”‚       â”œâ”€â”€ computacion_neuromorfica/
â”‚       â””â”€â”€ qkd/
â”œâ”€â”€ docs/                  # DocumentaciÃ³n
â”‚   â”œâ”€â”€ CLI_AGENT_MANUAL.md
â”‚   â””â”€â”€ ADDING_DOCUMENTS.md
â”œâ”€â”€ indices/               # Ãndices generados
â”‚   â”œâ”€â”€ qdrant/           # Base de datos vectorial
â”‚   â”œâ”€â”€ bm25_index.pkl    # Ãndice BM25
â”‚   â”œâ”€â”€ chunks.pkl        # AlmacÃ©n de chunks
â”‚   â””â”€â”€ manifest.json     # Tracking de documentos
â”œâ”€â”€ logs/                  # Logs y costes
â”‚   â””â”€â”€ cost_tracking.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/        # Parsing y chunking
â”‚   â”‚   â”œâ”€â”€ parser.py     # Parser de Markdown
â”‚   â”‚   â”œâ”€â”€ chunker.py    # Chunking jerÃ¡rquico
â”‚   â”‚   â””â”€â”€ indexer.py    # IndexaciÃ³n
â”‚   â”œâ”€â”€ retrieval/        # RecuperaciÃ³n
â”‚   â”‚   â”œâ”€â”€ vector_retriever.py
â”‚   â”‚   â”œâ”€â”€ bm25_retriever.py
â”‚   â”‚   â”œâ”€â”€ graph_retriever.py
â”‚   â”‚   â””â”€â”€ fusion.py     # FusiÃ³n hÃ­brida (RRF)
â”‚   â”œâ”€â”€ generation/       # GeneraciÃ³n
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py
â”‚   â”‚   â”œâ”€â”€ synthesizer.py
â”‚   â”‚   â””â”€â”€ citation_injector.py
â”‚   â”œâ”€â”€ agents/           # Agentes inteligentes
â”‚   â”‚   â”œâ”€â”€ router.py     # Routing de queries
â”‚   â”‚   â”œâ”€â”€ planner.py    # PlanificaciÃ³n multi-hop
â”‚   â”‚   â””â”€â”€ critic.py     # CrÃ­tica de respuestas
â”‚   â”œâ”€â”€ execution/        # EjecuciÃ³n de cÃ³digo
â”‚   â”‚   â””â”€â”€ sandbox.py    # Sandbox seguro
â”‚   â”œâ”€â”€ utils/            # Utilidades
â”‚   â”‚   â””â”€â”€ cost_tracker.py
â”‚   â””â”€â”€ cli/              # Interfaz de lÃ­nea de comandos
â”‚       â”œâ”€â”€ ask_library.py
â”‚       â””â”€â”€ ingest_library.py
â”œâ”€â”€ outputs/              # Sesiones guardadas
â”œâ”€â”€ .venv/                # Entorno virtual Python
â”œâ”€â”€ .env                  # API keys (no commitear)
â””â”€â”€ README.md
```

## âš™ï¸ ConfiguraciÃ³n

### settings.yaml

```yaml
embedding:
  provider: openai
  model: text-embedding-3-large
  dimensions: 3072

chunking:
  micro_size: 200
  meso_size: 512
  macro_size: 2048

retrieval:
  vector_top_k: 30
  bm25_top_k: 30
  rrf_k: 60
  final_top_k: 10
  # Pesos dinÃ¡micos ajustados automÃ¡ticamente segÃºn tipo de query:
  # - Query exacta (BB84, Shor): bm25=0.6, vector=0.3, graph=0.1
  # - Query conceptual: vector=0.5, bm25=0.3, graph=0.2
  # - Query relacional: graph=0.5, vector=0.3, bm25=0.2
  # - Query comparativa: vector=0.4, bm25=0.3, graph=0.3

generation:
  provider: anthropic
  model: claude-sonnet-4-5-20250929
  temperature: 0.3
  max_tokens: 2000
```

## ğŸ“Š Comandos de IndexaciÃ³n

```bash
# IndexaciÃ³n incremental (solo nuevos/modificados)
python -m src.cli.ingest_library

# Reindexar todo
python -m src.cli.ingest_library --force

# Ver estadÃ­sticas
python -m src.cli.ingest_library --stats

# Construir grafo de conocimiento
python -m src.cli.ingest_library --build-graph

# Dry run (ver quÃ© se procesarÃ­a)
python -m src.cli.ingest_library --dry-run

# Chunking semÃ¡ntico (detecta definiciones, teoremas, demostraciones)
python -m src.cli.ingest_library --semantic-chunking --force

# IndexaciÃ³n paralela (3-5x mÃ¡s rÃ¡pido, activado por defecto)
python -m src.cli.ingest_library --force --workers 8

# Desactivar paralelizaciÃ³n (modo secuencial)
python -m src.cli.ingest_library --no-parallel
```

### IndexaciÃ³n Paralela

Por defecto, la indexaciÃ³n usa procesamiento paralelo para acelerar la generaciÃ³n de embeddings:

```bash
# Usar mÃ¡s workers (default: 4)
python -m src.cli.ingest_library --workers 8

# Ajustar batch size por worker
python -m src.cli.ingest_library --batch-size 100

# Desactivar para debugging
python -m src.cli.ingest_library --no-parallel
```

| Workers | Speedup tÃ­pico | Caso de uso                  |
| ------- | -------------- | ---------------------------- |
| 1       | 1x (baseline)  | Debugging, lÃ­mite de rate    |
| 4       | 2.5-3x         | Default, API estÃ¡ndar        |
| 8       | 3.5-4x         | API tier alto, reindexaciÃ³n  |
| 16      | 4-5x           | API enterprise, batch masivo |

> âš ï¸ Nota: Demasiados workers pueden causar rate limiting en APIs. Ajusta segÃºn tu tier.

### Chunking SemÃ¡ntico Adaptativo

El flag `--semantic-chunking` activa la detecciÃ³n automÃ¡tica de lÃ­mites semÃ¡nticos naturales:

| Bloque Detectado        | DescripciÃ³n                            | PreservaciÃ³n        |
| ----------------------- | -------------------------------------- | ------------------- |
| **DefiniciÃ³n**          | `**DefiniciÃ³n X:**`                    | AtÃ³mico             |
| **Teorema/Lema**        | `**Teorema X:**`, `**Lema X:**`        | AtÃ³mico             |
| **DemostraciÃ³n**        | `**DemostraciÃ³n:**` hasta `â–¡`          | Divisible por pasos |
| **Ejemplo**             | `**Ejemplo X:**`                       | AtÃ³mico             |
| **Algoritmo/Protocolo** | `**Algoritmo X:**`, `**Protocolo X:**` | AtÃ³mico             |
| **CÃ³digo**              | Bloques ` ``` `                        | AtÃ³mico             |
| **Ecuaciones**          | Bloques `$$...$$`                      | AtÃ³mico             |

Beneficios:

- Evita cortar definiciones o teoremas a la mitad
- Mantiene contexto semÃ¡ntico completo
- Mejora la relevancia de chunks recuperados

## ğŸ’° EstimaciÃ³n de Costes

### IndexaciÃ³n (una vez)

- **Embeddings**: ~$0.13 / 1M tokens (text-embedding-3-large)
- Biblioteca tÃ­pica (50 papers): ~$0.50-1.00 total

### Consultas

- **Claude Sonnet 4.5**: $3/1M input, $15/1M output
- **GPT-4.1-mini**: $0.40/1M input, $1.60/1M output
- Consulta tÃ­pica: $0.01-0.05

## ğŸ”§ Troubleshooting

### Error: "OPENAI_API_KEY no configurada"

```bash
cp .env.example .env
# Editar .env con tu API key
```

### Error: "No se encontraron Ã­ndices"

```bash
python -m src.cli.ingest_library
```

### Error: "qdrant-client no instalado"

```bash
pip install qdrant-client
```

### Memoria insuficiente

- Reducir `batch_size` en settings.yaml
- Usar embeddings de menor dimensiÃ³n

## ğŸ“š AÃ±adir documentos

1. Convierte tus PDFs a Markdown (usando herramientas como `marker`, `nougat`, etc.)
2. Crea una subcarpeta temÃ¡tica si no existe:
   - `data/books/tu_tematica/nombre_libro/`
   - `data/papers/tu_tematica/nombre_paper/`
3. Coloca el `.md` y opcionalmente el `.pdf` original
4. Ejecuta `python -m src.cli.ingest_library`

### Estructura recomendada

```
data/books/
â”œâ”€â”€ algebra_lineal/
â”‚   â””â”€â”€ mi_libro_algebra/
â”‚       â”œâ”€â”€ mi_libro_algebra.md
â”‚       â””â”€â”€ mi_libro_algebra.pdf  (opcional)
â”œâ”€â”€ analisis_matematico/
â”œâ”€â”€ fisica_clasica/
â””â”€â”€ ...
```

## ğŸ¤ Estructura de Markdown recomendada

```markdown
# TÃ­tulo del Documento

## CapÃ­tulo 1: IntroducciÃ³n

### 1.1 Conceptos bÃ¡sicos

Contenido...

### 1.2 FÃ³rmulas importantes

La ecuaciÃ³n de SchrÃ¶dinger:

$$i\hbar\frac{\partial}{\partial t}\Psi = \hat{H}\Psi$$

## CapÃ­tulo 2: ...
```

## ğŸ“ Licencia

Uso personal/educativo. Los contenidos de la biblioteca son propiedad de sus respectivos autores.

---

## ğŸ“Š EstadÃ­sticas de la Biblioteca (Enero 2026)

| CategorÃ­a  | Contenido                     | Palabras       |
| ---------- | ----------------------------- | -------------- |
| **Libros** | 33 libros en 10 categorÃ­as    | ~5.3M          |
| **Papers** | 10 papers en 2 categorÃ­as     | ~160K          |
| **Total**  | 43 documentos, ~56,000 chunks | ~5.5M palabras |

### Desglose por Ã¡rea:

- ğŸ”¢ **Estructuras Algebraicas**: 8 libros (~1.6M palabras)
- ğŸ“ **GeometrÃ­as Lineales**: 6 libros (~788K palabras)
- ğŸ”µ **TopologÃ­a**: 3 libros (~554K palabras)
- âš›ï¸ **ComputaciÃ³n CuÃ¡ntica**: 4 libros (~713K palabras)
- ğŸŒ€ **MecÃ¡nica CuÃ¡ntica**: 2 libros (~419K palabras)
- ğŸ“¡ **InformaciÃ³n CuÃ¡ntica**: 2 libros (~463K palabras)
- ğŸ§  **ComputaciÃ³n NeuromÃ³rfica**: 1 libro + 3 papers
- ğŸ” **QKD/CriptografÃ­a CuÃ¡ntica**: 7 papers (~129K palabras)

---

**Autor**: Desarrollado para el MÃ¡ster en ComputaciÃ³n CuÃ¡ntica - UNIR  
**Ãšltima actualizaciÃ³n**: Enero 2026
